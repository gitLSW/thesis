{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "365d2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import re\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294746c",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95feef5",
   "metadata": {},
   "source": [
    "1. Identification and merging of relevant messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99a866ec-c707-4b14-a4b6-446e5213f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read contents files\n",
    "df_2023_contents = pd.read_csv(\"~/Desktop/Tickets/2023/contents.csv\")\n",
    "df_2023_contents = pd.read_csv(\"~/Desktop/Tickets/2023/contents.csv\")\n",
    "df_2021_contents = pd.read_csv(\"~/Desktop/Tickets/2021/contents.csv\")\n",
    "df_2022_contents = pd.read_csv(\"~/Desktop/Tickets/2022/contents.csv\", on_bad_lines='skip')\n",
    "df_2020_contents = pd.read_csv(\"~/Desktop/Tickets/2020/contents.csv\")\n",
    "df_2019_contents = pd.read_csv(\"~/Desktop/Tickets/2019/contents.csv\")\n",
    "df_2018_contents = pd.read_csv(\"~/Desktop/Tickets/2018/contents.csv\")\n",
    "df_2017_contents = pd.read_csv(\"~/Desktop/Tickets/2017/contents.csv\")\n",
    "df_2016_contents = pd.read_csv(\"~/Desktop/Tickets/2016/contents.csv\")\n",
    "df_2015_contents = pd.read_csv(\"~/Desktop/Tickets/2015/contents.csv\")\n",
    "\n",
    "# Merge content information\n",
    "df_result = pd.concat([df_2015_contents,df_2016_contents,df_2017_contents,df_2018_contents,df_2019_contents,df_2020_contents,df_2021_contents,df_2022_contents,df_2023_contents])\n",
    "df_result = df_result.map(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp column\n",
    "df_result['timestamp'] = df_result['Datum'] + ' ' + df_result['Uhrzeit']\n",
    "df_result['timestamp'] = pd.to_datetime(df_result['timestamp'])\n",
    "df_result = df_result.drop(columns=['Datum', 'Uhrzeit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7579/521410608.py:7: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_result = df_result.groupby('ID', group_keys=False).apply(filter_group)\n"
     ]
    }
   ],
   "source": [
    "# Only keep the first message of a ticket and all messages sent within the first 3 minutes.\n",
    "def filter_group(group):\n",
    "    min_time = group['timestamp'].min()\n",
    "    threshold_time = min_time + timedelta(minutes=3)\n",
    "    return group[group['timestamp'] <= threshold_time]\n",
    "\n",
    "df_result = df_result.groupby('ID', group_keys=False).apply(filter_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62fbe144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude irrelevant messages\n",
    "df_result = df_result[~((df_result['Nachrichtentyp'] == 'LÃ¶sung') | (df_result['Nachrichtentyp'] == 'Interne Notiz') | (df_result['Nachrichtentyp'] == 'Systemdaten'))]\n",
    "\n",
    "# Only keep U-Users and H-Users \n",
    "df_result = df_result[df_result['Absender'].str.startswith(('H', 'U') , na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30bb1a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7579/771059327.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_result = df_result.groupby('ID').apply(merge_rows).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Merge ticket texts\n",
    "def merge_rows(group):\n",
    "    merged_text = ' '.join(group['Text'])\n",
    "    row_with_smallest_timestamp = group.loc[group['timestamp'].idxmin()].copy()\n",
    "    row_with_smallest_timestamp['Text'] = merged_text\n",
    "    return row_with_smallest_timestamp\n",
    "\n",
    "df_result = df_result.groupby('ID').apply(merge_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d8f1a",
   "metadata": {},
   "source": [
    "2. Adding ticket information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72ca506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tickets files\n",
    "df_2023_tickets = pd.read_csv(\"~/Desktop/Tickets/2023/tickets.csv\")\n",
    "df_2021_tickets = pd.read_csv(\"~/Desktop/Tickets/2021/tickets.csv\")\n",
    "df_2022_tickets = pd.read_csv(\"~/Desktop/Tickets/2022/tickets.csv\")\n",
    "df_2020_tickets = pd.read_csv(\"~/Desktop/Tickets/2020/tickets.csv\")\n",
    "df_2019_tickets = pd.read_csv(\"~/Desktop/Tickets/2019/tickets.csv\")\n",
    "df_2018_tickets = pd.read_csv(\"~/Desktop/Tickets/2018/tickets.csv\")\n",
    "df_2017_tickets = pd.read_csv(\"~/Desktop/Tickets/2017/tickets.csv\")\n",
    "df_2016_tickets = pd.read_csv(\"~/Desktop/Tickets/2016/tickets.csv\")\n",
    "df_2015_tickets = pd.read_csv(\"~/Desktop/Tickets/2015/tickets.csv\")\n",
    "\n",
    "# Merge ticket information\n",
    "df_tickets = pd.concat([df_2015_tickets,df_2016_tickets,df_2017_tickets,df_2018_tickets,df_2019_tickets,df_2020_tickets,df_2021_tickets,df_2022_tickets,df_2023_tickets])\n",
    "df_tickets = df_tickets.map(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1e736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join\n",
    "df_result = pd.merge(df_result, df_tickets, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file from Janik to add verified ticket labels\n",
    "df_labels = pd.read_csv(\"~/Desktop/Tickets/ticket_labels_JSc.csv\")\n",
    "df_labels = df_labels.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_labels.drop(index=[7531,7532,7533,7534], inplace=True)\n",
    "df_labels['ID'] = df_labels['ID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left outer join\n",
    "df_result = pd.merge(df_result, df_labels, on='ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387501b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = df_result.drop(df_result[df_result['support_level'] == 'Not relevant'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7579/3762762705.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_result['support_level'] = df_result['support_level'].replace(mapping)\n",
      "/tmp/ipykernel_7579/3762762705.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_result['Ticket Label'] = df_result['Ticket Label'].replace(mapping)\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistency of label values\n",
    "\n",
    "# support_level\n",
    "mapping = {'1st Level': 1, '2nd Level': 2}\n",
    "df_result['support_level'] = df_result['support_level'].replace(mapping)\n",
    "\n",
    "# Ticket Label\n",
    "mapping = {'': None, '1. Level': 1, '2. Level': 2}\n",
    "df_result['Ticket Label'] = df_result['Ticket Label'].replace(mapping)\n",
    "\n",
    "# Abteilung Label\n",
    "mapping = {'Vertrag': 'Contract', 'Applikation': 'Application', '': None}\n",
    "df_result['Abteilung Label'] = df_result['Abteilung Label'].replace(mapping)\n",
    "\n",
    "# Product\n",
    "mapping = {'': None, 'Business By Design': 'Business by Design', 'Entwicklungssystem bzw. Mandant': 'Entwicklungssystem/Mandant'}\n",
    "df_result['Produkt Label'] = df_result['Produkt Label'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff339e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask where all specified columns are NaN\n",
    "mask = df_result['support_level'].isna() & df_result['department'].isna() & df_result['product'].isna()\n",
    "\n",
    "# Use loc to target the rows and columns where the mask is True and replace them\n",
    "df_result.loc[mask, 'support_level'] = df_result.loc[mask, 'Ticket Label']\n",
    "df_result.loc[mask, 'department'] = df_result.loc[mask, 'Abteilung Label']\n",
    "df_result.loc[mask, 'product'] = df_result.loc[mask, 'Produkt Label']\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_result = df_result.drop(columns=['Ticket Label', 'Abteilung Label', 'Produkt Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a98e5",
   "metadata": {},
   "source": [
    "3. Merging the ticket text with the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8dcf527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge text and description into text\n",
    "df_result['Text'] = df_result['Text'].str.cat(df_result['Beschreibung'], sep=' ')\n",
    "df_result = df_result.drop(columns=['Beschreibung'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c11c8",
   "metadata": {},
   "source": [
    "4. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7343b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows\n",
    "df_result = df_result[~((df_result['support_level'].isna()) & df_result['department'].isna() & df_result['product'].isna())]\n",
    "df_result = df_result[~((df_result['support_level'] == 2) & df_result['department'].isna() & df_result['product'].isna())]\n",
    "df_result = df_result[~((df_result['support_level'] == 2) & (df_result['department'] == 'Application') & df_result['product'].isna())] # new\n",
    "df_result = df_result[~((df_result['support_level'] == 2) & (df_result['department'] == 'Basis') & df_result['product'].isna())] # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d12f384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with none\n",
    "mask = (df_result['support_level'] == 1) & ((df_result['department'].notna()) | (df_result['product'].notna()))\n",
    "df_result.loc[mask, ['department', 'product']] = None\n",
    "\n",
    "mask = (df_result['support_level'] == 2) & (df_result['department'] == 'Contract') & (df_result['product'].notna()) # new\n",
    "df_result.loc[mask, ['department', 'product']] = None # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4906a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '\\n' symbols from text\n",
    "df_result['Text'] = df_result['Text'].str.replace('\\n', ' ')\n",
    "\n",
    "# Convert the text to lowercase\n",
    "df_result['Text'] = df_result['Text'].str.lower()\n",
    "\n",
    "# Remove URLs\n",
    "df_result['Text'] = df_result['Text'].str.replace(r'(http[s]?://|www\\.)(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<url>', regex=True)\n",
    "\n",
    "# Remove email addresses\n",
    "df_result['Text'] = df_result['Text'].str.replace(r'\\S+@\\S+', '<email>', regex=True)\n",
    "\n",
    "# Strip leading and trailing white spaces from the text\n",
    "df_result['Text'] = df_result['Text'].str.strip()\n",
    "\n",
    "# Remove extra spaces from the text\n",
    "df_result['Text'] = df_result['Text'].str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f9bb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant columns and rename columns\n",
    "df_result = df_result[['ID','Absender','Text','Kategorie ID','Unterkategorie ID','timestamp','support_level','department','product']]\n",
    "df_result = df_result.rename(columns={'ID': 'id', 'Absender':'sender', 'Text': 'text','Beschreibung': 'description','Kategorie ID':'category','Unterkategorie ID':'subcategory','support_level':'level'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3ab0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv('~/Desktop/Tickets/data_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5754f",
   "metadata": {},
   "source": [
    "Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2946727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "2.0    5387\n",
       "1.0    2513\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result['level'].value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65082d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create a mask where 'support_level' equals 2, 'department' equals 'Contract', and 'product' is NaN\n",
    "mask = (df_result['level'] == 2) & (df_result['department'] == 'Application') & (df_result['product'].isna())\n",
    "\n",
    "# Count the number of rows where the mask is True\n",
    "count_rows = mask.sum()\n",
    "\n",
    "print(count_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection (FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "lid.176.ftz cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlid.176.ftz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_language_with_probability\u001b[39m(text, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[1;32m      4\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(text, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ucc/lib/python3.12/site-packages/fasttext/FastText.py:441\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m eprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ucc/lib/python3.12/site-packages/fasttext/FastText.py:98\u001b[0m, in \u001b[0;36m_FastText.__init__\u001b[0;34m(self, model_path, args)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mfasttext()\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: lid.176.ftz cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model('lid.176.ftz')\n",
    "\n",
    "def detect_language_with_probability(text, threshold=0.7):\n",
    "    predictions = model.predict(text, k=1)\n",
    "    language, probability = predictions[0][0], predictions[1][0]\n",
    "    \n",
    "    if probability >= threshold:\n",
    "        return (language.replace('__label__', ''), probability)  # Extract language code and probability\n",
    "    else:\n",
    "        return ('unknown', probability)\n",
    "\n",
    "df_result['language_probability'] = df_result['text'].apply(detect_language_with_probability)\n",
    "df_result[['language', 'probability']] = pd.DataFrame(df_result['language_probability'].tolist(), index=df_result.index)\n",
    "\n",
    "df_result.drop(columns=['language_probability'], inplace=True)\n",
    "df_result['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423185b9",
   "metadata": {},
   "source": [
    "## Strategies ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370cea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove names\n",
    "def remove_names(row):\n",
    "    # Split the 'sender' to extract names\n",
    "    parts = row['sender'].split()\n",
    "    surname, lastname = parts[1], parts[2]\n",
    "    \n",
    "    surname_pattern = re.compile(re.escape(surname), re.IGNORECASE)\n",
    "    lastname_pattern = re.compile(re.escape(lastname), re.IGNORECASE)\n",
    "    \n",
    "    cleaned_text = surname_pattern.sub('<surname>', row['text'])\n",
    "    cleaned_text = lastname_pattern.sub('<lastname>', cleaned_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df_result['text'] = df_result.apply(remove_names, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e961a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Global Bike                                  1712\n",
       "ERPsim                                        639\n",
       "IDES                                          453\n",
       "HANA                                          414\n",
       "Sonstiges                                     392\n",
       "SAP Business Warehouse & Business Objects     311\n",
       "Entwicklungssystem/Mandant                    196\n",
       "TS410                                         155\n",
       "SAP4School                                    112\n",
       "Business by Design                             37\n",
       "UCC Portal                                     30\n",
       "Lumira                                         23\n",
       "Celonis                                        10\n",
       "UCC Hardware                                    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge GBI and GBS\n",
    "merged_category = 'Global Bike'\n",
    "categories_to_merge = ['GBI', 'GBS/Digital Transformation Curriculum']\n",
    "df_result['product'] = df_result['product'].replace(categories_to_merge, merged_category)\n",
    "df_result['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9f8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Global Bike                                  1712\n",
       "ERPsim                                        639\n",
       "HANA                                          414\n",
       "Sonstiges                                     392\n",
       "SAP Business Warehouse & Business Objects     311\n",
       "Entwicklungssystem/Mandant                    196\n",
       "TS410                                         155\n",
       "SAP4School                                    112\n",
       "Business by Design                             37\n",
       "UCC Portal                                     30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove legacy products\n",
    "values_to_drop = ['Lumira', 'Celonis', 'IDES'] \n",
    "df_result = df_result[~df_result['product'].isin(values_to_drop)]\n",
    "df_result['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3b7a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Global Bike                                  1712\n",
       "ERPsim                                        639\n",
       "HANA                                          414\n",
       "SAP Business Warehouse & Business Objects     311\n",
       "Entwicklungssystem/Mandant                    196\n",
       "TS410                                         155\n",
       "SAP4School                                    112\n",
       "Business by Design                             37\n",
       "UCC Portal                                     30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Sonstiges class\n",
    "values_to_drop = ['Sonstiges'] \n",
    "df_result = df_result[~df_result['product'].isin(values_to_drop)]\n",
    "df_result['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f68ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
